# Crawl Troubleshooting Guide

## Understanding "0 Pages Crawled"

When a crawl job shows **"0 crawled"** and **"0 indexed"** but has a status of **"completed"**, it means the crawler attempted to fetch the URL(s) but was unable to successfully retrieve any content.

## Common Reasons for 0 Pages Crawled

### 1. **Bot Protection / Cloudflare Challenge** â›”
**Most Common Reason**

Many modern websites use bot protection services (Cloudflare, Akamai, etc.) that block automated crawlers.

**Example:**
```bash
$ curl -I https://www.namecheap.com/
HTTP/2 403 Forbidden
cf-mitigated: challenge
```

**Affected Sites in Your Database:**
- namecheap.com - Returns HTTP 403 (Cloudflare protected)
- linkedin.com - Blocks bots aggressively
- chatgpt.com - Requires authentication + bot protection

**Signs:**
- HTTP 403 (Forbidden) response
- HTTP 503 with challenge page
- Cloudflare headers: `cf-mitigated: challenge`
- JavaScript challenges that require browser execution

**Solution:**
- These sites intentionally block crawlers
- Would require headless browser (Puppeteer/Playwright)
- May violate the site's Terms of Service
- Consider alternative data sources or APIs

---

### 2. **robots.txt Disallows Crawling** ðŸš«

The site's robots.txt file explicitly blocks your bot's User-Agent.

**Example:**
```
User-agent: EngineSearchBot
Disallow: /
```

**How to Check:**
```bash
curl https://example.com/robots.txt
```

**Solution:**
- Respect robots.txt (best practice)
- Check if your User-Agent is blocked
- Some sites allow specific bot names

---

### 3. **Authentication Required** ðŸ”

Site requires login or authentication to access content.

**Examples:**
- chatgpt.com (requires login)
- twitter.com/x.com (most content requires login)
- Private corporate sites

**Solution:**
- Cannot crawl without authentication
- Consider using official APIs if available

---

### 4. **Rate Limiting / IP Blocking** â±ï¸

Your IP was blocked due to too many requests.

**Signs:**
- HTTP 429 (Too Many Requests)
- HTTP 403 after multiple requests
- Temporary blocks that clear after time

**Your Crawler Settings:**
```
CRAWLER_REQUESTS_PER_SECOND=2
CRAWLER_MIN_DELAY_MS=1000
```

**Solution:**
- Already configured with conservative rate limiting
- Some sites still block even slow crawlers
- Consider rotating IPs (not recommended for production)

---

### 5. **Network/Connection Errors** ðŸŒ

Temporary network issues or DNS failures.

**Examples:**
- DNS resolution failed
- Connection timeout
- SSL/TLS errors
- Server temporarily down

**How to Check:**
```bash
# Test connectivity
curl -v https://example.com/

# Check DNS
nslookup example.com
```

**Solution:**
- These are usually temporary
- Crawler should retry (currently configured with exponential backoff)

---

### 6. **Invalid URL or Permanent Redirect** ðŸ”„

URL doesn't exist or redirects to uncrawlable location.

**Examples:**
- HTTP 404 (Not Found)
- HTTP 410 (Gone)
- Redirect to login page
- Redirect loop

**Solution:**
- Verify URL is correct
- Check if site moved to new domain

---

### 7. **JavaScript-Heavy Sites** ðŸ’»

Site requires JavaScript execution to render content.

**Examples:**
- Modern SPAs (React, Vue, Angular)
- Sites that load content dynamically via API calls

**Your Crawler:**
- Currently does **NOT** execute JavaScript
- Only fetches static HTML

**Solution:**
- Would need headless browser integration (Phase 6.7 - optional)
- Significantly increases complexity and resource usage

---

## How to Diagnose Your Specific Case

### Check the Database:

```sql
-- Get crawl history with errors
SELECT id, urls, status, pages_crawled, pages_indexed, error_message
FROM crawl_history
WHERE pages_crawled = 0
ORDER BY started_at DESC;

-- Check for error details (if logged)
SELECT * FROM crawl_errors
WHERE crawl_id = 'YOUR_JOB_ID';
```

### Manual Test:

```bash
# Test with your crawler's User-Agent
curl -A "EngineSearchBot/1.0 (+https://example.com/bot; bot@example.com)" \
     -I https://www.example.com/

# Check robots.txt
curl -A "EngineSearchBot/1.0" \
     https://www.example.com/robots.txt
```

---

## Sites That Successfully Crawled in Your Database

Looking at your crawl history, these sites worked well:

âœ… **github.com** - 6 pages crawled & indexed
âœ… **stripe.com** - 5 pages crawled & indexed
âœ… **algerie54.dz** - 1 page crawled & indexed
âœ… **clickup.com** - Successfully indexed

These sites are crawler-friendly and don't have aggressive bot protection.

---

## Sites That Failed (0 Pages)

âŒ **namecheap.com** - HTTP 403 (Cloudflare)
âŒ **linkedin.com** - Bot protection
âŒ **chatgpt.com** - Authentication required + bot protection
âŒ **freepik.com** - Likely bot protection
âŒ **contentful.com** - Likely bot protection
âŒ **nocodb.com** - Possible bot protection

---

## Recommendations

### For Production Use:

1. **Target Crawler-Friendly Sites:**
   - Small-medium business websites
   - Blogs and content sites
   - Public documentation
   - News sites (check robots.txt)

2. **Avoid:**
   - Social media platforms (LinkedIn, Twitter, Facebook)
   - E-commerce sites with heavy protection (Amazon, eBay)
   - SaaS platforms (Salesforce, HubSpot)
   - Sites requiring authentication

3. **Best Practices:**
   - Always respect robots.txt âœ… (Your crawler does this)
   - Use polite crawl delays âœ… (Your crawler does this)
   - Identify your bot clearly âœ… (Your crawler does this)
   - Provide contact information âœ… (In User-Agent)

4. **Monitor Crawl Success Rate:**
   ```sql
   -- Calculate success rate
   SELECT
       COUNT(*) as total_jobs,
       COUNT(*) FILTER (WHERE pages_crawled > 0) as successful,
       ROUND(100.0 * COUNT(*) FILTER (WHERE pages_crawled > 0) / COUNT(*), 2) as success_rate_percent
   FROM crawl_history;
   ```

---

## Improving Crawl Success Rate

### Option 1: Better Error Logging (Recommended)

Currently, errors aren't being logged to `crawl_errors` table. Implement error logging in worker to show:
- HTTP status codes
- Error messages
- Failed URLs

### Option 2: Add Retry Logic for Specific Errors

Already implemented with exponential backoff for:
- 408 (Request Timeout)
- 429 (Too Many Requests)
- 500, 502, 503, 504 (Server errors)

### Option 3: Headless Browser Support (Advanced)

For JavaScript-heavy sites:
- Add Puppeteer/Playwright integration
- Significantly increases resource usage
- May still be blocked by bot protection

### Option 4: Content API Partnerships

For sites you frequently need:
- Use official APIs instead of crawling
- More reliable and faster
- Often includes structured data

---

## Current Crawler Status

**Your Crawler Configuration:**
- User-Agent: `EngineSearchBot/1.0 (+https://example.com/bot; bot@example.com)`
- Rate Limit: 2 requests/second per domain
- Min Delay: 1000ms between requests
- Max Depth: Configurable (default 3)
- Respects robots.txt: âœ… Yes
- Circuit Breaker: âœ… Enabled
- Retry Logic: âœ… Exponential backoff

**Success Rate (from your 20 jobs):**
- Successful crawls: ~40% (8 out of 20 jobs)
- Failed crawls: ~60% (12 jobs with 0 pages)
- Common failure reason: Bot protection / Cloudflare

This is actually a **reasonable success rate** for a production crawler! Many major sites actively block bots.

---

## Next Steps

1. **Focus on Crawler-Friendly Sites:**
   - Target smaller business sites
   - Test with blogs and documentation sites
   - Avoid major tech companies

2. **Implement Better Error Logging:**
   - Log HTTP status codes
   - Store error details in `crawl_errors` table
   - Show errors in admin dashboard

3. **Test Before Crawling:**
   - Manual curl test before submitting crawl job
   - Check for HTTP 403/503 responses
   - Verify robots.txt allows crawling

4. **Consider Alternative Approaches:**
   - Use search engine APIs (Google Custom Search, Bing)
   - Partner with content providers
   - Focus on your own content or open datasets

---

## Testing a New Site

Before crawling, run these checks:

```bash
# 1. Test HTTP response
curl -A "EngineSearchBot/1.0" -I https://example.com/

# 2. Check robots.txt
curl https://example.com/robots.txt

# 3. Test with browser User-Agent (comparison)
curl -A "Mozilla/5.0" -I https://example.com/

# 4. Check for Cloudflare
curl -I https://example.com/ | grep -i cloudflare
```

If you get HTTP 200 with your bot User-Agent, the site should crawl successfully! âœ…
